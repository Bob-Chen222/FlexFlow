{
    "num_gpus": 4,
    "memory_per_gpu": 14000,
    "zero_copy_memory_per_gpu": 30000,
    "num_cpus": 4,
    "legion_utility_processors": 4,
    "data_parallelism_degree": 1,
    "tensor_parallelism_degree": 1,
    "pipeline_parallelism_degree": 4,
    "offload": false,
    "offload_reserve_space_size": 1048576,
    "use_4bit_quantization": false,
    "use_8bit_quantization": false,
    "profiling": false,
    "fusion": true,
    "llm_model": "decapoda-research/llama-7b-hf",
    "llm_weight": "",
    "llm_tokenizer": "",
    "clean_model_cache": false,
    "full_precision": true,
    "ssms": [
        {
            "ssm_model": "JackFram/llama-160m",
            "ssm_weight": "",
            "ssm_tokenizer": "",
            "clean_model_cache": false,
            "full_precision": true
        }
    ],
    "prompt": "../inference/prompt/test.json",
    "output_file": "../inference/output/spec_inference_llama.txt"
}