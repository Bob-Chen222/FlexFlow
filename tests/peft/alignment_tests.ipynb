{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_weight_base_path = \"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors\"\n",
    "ff_weight_base_path = \"/usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors\"\n",
    "def compare_tensors(hf_tensor_filepath, ff_tensor_filepath, tolerance=1e-2):\n",
    "    assert(os.path.exists(hf_tensor_filepath) and os.path.exists(ff_tensor_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor = ff_tensor[:len_hf_tensor]\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor, hf_tensor, atol=tolerance):\n",
    "        print(f\"mismatch between {hf_tensor_filepath} and {ff_tensor_filepath}\")\n",
    "        print(f\"HF: {hf_tensor}\\nFF:{ff_tensor}\")\n",
    "        print(np.isclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor, hf_tensor, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "        #print(np.nonzero(hf_tensor)[0])\n",
    "        # print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\n",
    "        # print(ff_tensor[36], hf_tensor[36])\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len_hf_tensor)\n",
    "    print(\"Ok!\")\n",
    "def compare_tensors_difference(hf_tensor_filepath, ff_tensor1_filepath, ff_tensor2_filepath, tolerance=1e-2):\n",
    "    assert(os.path.exists(hf_tensor_filepath))\n",
    "    assert(os.path.exists(ff_tensor1_filepath))\n",
    "    assert(os.path.exists(ff_tensor2_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_filepath, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor1 = ff_tensor1[:len_hf_tensor]\n",
    "    ff_tensor2 = ff_tensor2[:len_hf_tensor]\n",
    "    ff_tensor = ff_tensor1 - ff_tensor2\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor, hf_tensor, atol=tolerance):\n",
    "        print(f\"mismatch between {hf_tensor_filepath} and {ff_tensor1_filepath} - {ff_tensor2_filepath}\")\n",
    "        print(f\"HF: {hf_tensor}\\nFF:{ff_tensor}\")\n",
    "        print(np.isclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor, hf_tensor, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "        #print(np.nonzero(hf_tensor)[0])\n",
    "        # print(np.where(np.isclose(ff_tensor, hf_tensor, atol=tolerance) ==0)[0])\n",
    "        # print(ff_tensor[36], hf_tensor[36])\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len_hf_tensor)\n",
    "    print(\"Ok!\")\n",
    "def compare_hf_tensors(tensor1_fp, tensor2_fp):\n",
    "    assert(os.path.exists(tensor1_fp) and os.path.exists(tensor2_fp))\n",
    "    hf_tensor1 = torch.load(tensor1_fp)\n",
    "    hf_tensor2 = torch.load(tensor2_fp)\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    if type(hf_tensor2) == tuple or type(hf_tensor2) == list:\n",
    "        assert(len(hf_tensor2) == 1)\n",
    "        hf_tensor2 = hf_tensor2[0]\n",
    "    assert(torch.squeeze(hf_tensor1).shape == torch.squeeze(hf_tensor2).shape)\n",
    "    hf_tensor1 = torch.nan_to_num(hf_tensor1)\n",
    "    hf_tensor2 = torch.nan_to_num(hf_tensor2)\n",
    "    if not (np.allclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy())):\n",
    "        print(f\"mismatch between {tensor1_fp} and {tensor2_fp}\")\n",
    "        print(hf_tensor1)\n",
    "        print(hf_tensor2)\n",
    "        print(np.isclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy()))\n",
    "        mismatches = np.where(~np.isclose(hf_tensor1.detach().cpu().numpy(), hf_tensor2.detach().cpu().numpy()))[0]\n",
    "        print(mismatches)\n",
    "        assert(False)\n",
    "    print(\"Ok!\")\n",
    "\n",
    "def check_hf_sum_tensors(tensor_sum_fp, tensor1_fp, tensor2_fp):\n",
    "    assert(os.path.exists(tensor_sum_fp) and os.path.exists(tensor1_fp) and os.path.exists(tensor2_fp))\n",
    "    hf_tensor_sum = torch.load(tensor_sum_fp)\n",
    "    hf_tensor1 = torch.load(tensor1_fp)\n",
    "    hf_tensor2 = torch.load(tensor2_fp)\n",
    "    if type(hf_tensor_sum) == tuple or type(hf_tensor_sum) == list:\n",
    "        assert(len(hf_tensor_sum) == 1)\n",
    "        hf_tensor_sum = hf_tensor_sum[0]\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    if type(hf_tensor2) == tuple or type(hf_tensor2) == list:\n",
    "        assert(len(hf_tensor2) == 1)\n",
    "        hf_tensor2 = hf_tensor2[0]\n",
    "    assert(torch.squeeze(hf_tensor_sum).shape == torch.squeeze(hf_tensor1).shape)\n",
    "    assert(torch.squeeze(hf_tensor1).shape == torch.squeeze(hf_tensor2).shape)\n",
    "    hf_tensor1 = torch.nan_to_num(hf_tensor1)\n",
    "    hf_tensor2 = torch.nan_to_num(hf_tensor2)\n",
    "    hf_tensor_sum = torch.nan_to_num(hf_tensor_sum)\n",
    "    sum_check_tensor = hf_tensor1 + hf_tensor2\n",
    "    if not (np.allclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy())):\n",
    "        print(f\"mismatch between {sum_check_tensor} and {tensor1_fp} + {tensor2_fp}\")\n",
    "        print(tensor_sum_fp)\n",
    "        print(sum_check_tensor)\n",
    "        print(hf_tensor1)\n",
    "        print(hf_tensor2)\n",
    "        print(np.isclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy()))\n",
    "        mismatches = np.where(~np.isclose(sum_check_tensor.detach().cpu().numpy(), hf_tensor_sum.detach().cpu().numpy()))[0]\n",
    "        print(mismatches)\n",
    "        assert(False)\n",
    "    print(\"Ok!\")\n",
    "def check_hf_zero_tensor(hf_tensor_fp):\n",
    "    assert(os.path.exists(hf_tensor_fp))\n",
    "    hf_tensor1 = torch.load(hf_tensor_fp)\n",
    "    if type(hf_tensor1) == tuple or type(hf_tensor1) == list:\n",
    "        assert(len(hf_tensor1) == 1)\n",
    "        hf_tensor1 = hf_tensor1[0]\n",
    "    assert(torch.count_nonzero(torch.nan_to_num(hf_tensor1)).sum() == 0)\n",
    "def print_tensors(hf_tensor_filepath, ff_tensor_filepath, txt=\"\"):\n",
    "    assert(os.path.exists(hf_tensor_filepath) and os.path.exists(ff_tensor_filepath))\n",
    "    hf_tensor = torch.load(hf_tensor_filepath)\n",
    "    if type(hf_tensor) == tuple or type(hf_tensor) == list:\n",
    "        assert(len(hf_tensor) == 1)\n",
    "        hf_tensor = hf_tensor[0]\n",
    "    hf_tensor = torch.nan_to_num(hf_tensor)\n",
    "    hf_tensor = hf_tensor.flatten().detach().cpu().numpy()\n",
    "    ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "    len_hf_tensor = hf_tensor.shape[0]\n",
    "    ff_tensor = ff_tensor[:len_hf_tensor]\n",
    "\n",
    "    print(f\"{txt} - HF tensor:\")\n",
    "    print(hf_tensor)\n",
    "    print(f\"{txt} - FF tensor: \")\n",
    "    print(ff_tensor)\n",
    "def compare_flexflow_tensors(ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5, max_len=-1):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "\n",
    "    if (ff_tensor1.shape != ff_tensor2.shape):\n",
    "        print(ff_tensor1.shape, ff_tensor2.shape)\n",
    "    assert(ff_tensor1.shape == ff_tensor2.shape)\n",
    "\n",
    "    if max_len > -1:\n",
    "        ff_tensor1 = ff_tensor1[:max_len]\n",
    "        ff_tensor2 = ff_tensor2[:max_len]\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor1, ff_tensor2, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor1_fp} and {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")\n",
    "def compare_flexflow_tensors_shortest(ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "    minlen = min(ff_tensor1.shape[0], ff_tensor2.shape[0])\n",
    "    ff_tensor1 = ff_tensor1[:minlen]\n",
    "    ff_tensor2 = ff_tensor2[:minlen]\n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor1, ff_tensor2, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor1_fp} and {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor1, ff_tensor2, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")\n",
    "def check_flexflow_tensors_sum(ff_tensor_sum_fp, ff_tensor1_fp, ff_tensor2_fp, tolerance=1e-5):\n",
    "    assert(os.path.exists(ff_tensor1_fp) and os.path.exists(ff_tensor2_fp))\n",
    "    ff_tensor1 = np.loadtxt(ff_tensor1_fp, delimiter=',')\n",
    "    ff_tensor2 = np.loadtxt(ff_tensor2_fp, delimiter=',')\n",
    "    ff_tensor_sum = np.loadtxt(ff_tensor_sum_fp, delimiter=',')\n",
    "    \n",
    "    ff_sum = ff_tensor1 + ff_tensor2\n",
    "    assert(ff_tensor1.shape == ff_tensor2.shape)\n",
    "    \n",
    "    mismatches = []\n",
    "    if not np.allclose(ff_tensor_sum, ff_sum, atol=tolerance):\n",
    "        print(f\"mismatch between {ff_tensor_sum_fp} and sum of {ff_tensor1_fp} + {ff_tensor2_fp}\")\n",
    "        print(f\"Tensor1: {ff_tensor1}\\nTensor2:{ff_tensor2}\")\n",
    "        print(f\"Sum Tensor: {ff_tensor_sum}\\nActual sum:{ff_sum}\")\n",
    "        print(np.isclose(ff_tensor_sum, ff_sum, atol=tolerance))\n",
    "        mismatches = np.where(~np.isclose(ff_tensor_sum, ff_sum, atol=tolerance))[0]\n",
    "        print(mismatches)\n",
    "    #assert(np.allclose(ff_tensor, hf_tensor, atol=tolerance))\n",
    "    assert(len(mismatches) <= .05*len(ff_tensor1))\n",
    "    print(\"Ok!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for layer_num in range(tot_num_layers):\n",
    "    hf_input_ln_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.input_layernorm.output_0\"\n",
    "    ff_input_ln_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_RMSNorm_shard-id_0_output_0\"\n",
    "    if layer_num > 0:\n",
    "        ff_input_ln_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_norm_shard-id_0_output_1\"\n",
    "    compare_tensors(hf_input_ln_out, ff_input_ln_out)\n",
    "    hf_attn_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.o_proj.output_0\"\n",
    "    ff_attn_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_attn_out, ff_attn_out)\n",
    "    hf_ffn_norm_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.post_attention_layernorm.output_0\"\n",
    "    ff_ffn_norm_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_output_1\"\n",
    "    compare_tensors(hf_ffn_norm_out, ff_ffn_norm_out)\n",
    "    # w1\n",
    "    hf_gate_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.gate_proj.output_0\"\n",
    "    ff_gate_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_gate_proj_out, ff_gate_proj_out)\n",
    "    # w3\n",
    "    hf_up_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.up_proj.output_0\" \n",
    "    ff_up_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_up_proj_out, ff_up_proj_out)\n",
    "    # w2\n",
    "    hf_down_proj_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.input_0\"\n",
    "    hf_down_proj_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.output_0\"\n",
    "    ff_down_proj_in = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_input_0\"\n",
    "    ff_down_proj_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_output_0\"\n",
    "    compare_tensors(hf_down_proj_in, ff_down_proj_in)\n",
    "    # compare_tensors(hf_down_proj_out, ff_down_proj_out)\n",
    "    # LORA input\n",
    "    hf_lora_A_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.input_0\"\n",
    "    ff_lora_A_in = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_input_0\"\n",
    "    compare_hf_tensors(hf_down_proj_in, hf_lora_A_in)\n",
    "    compare_tensors(hf_lora_A_in, ff_lora_A_in)\n",
    "    # LORA weights\n",
    "    hf_lora_A_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp)\n",
    "    hf_lora_B_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp)\n",
    "    # LORA intermediate hf\n",
    "    hf_lora_A_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.output_0\"\n",
    "    hf_lora_B_in = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.input_0\"\n",
    "    compare_hf_tensors(hf_lora_A_out, hf_lora_B_in)\n",
    "    # LORA output\n",
    "    hf_lora_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.output_0\"\n",
    "    ff_lora_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_output_0\"\n",
    "    # compare_tensors(hf_lora_out, ff_lora_out)\n",
    "    # compare_flexflow_tensors(ff_down_proj_out, ff_lora_out)\n",
    "    # compare_tensors(hf_down_proj_out, ff_lora_out)\n",
    "    compare_tensors_difference(hf_lora_out, ff_lora_out, ff_down_proj_out)\n",
    "    \n",
    "\n",
    "# After last layer only\n",
    "hf_norm_out = f\"{hf_weight_base_path}/fwd_step_0_norm.output_0\"\n",
    "ff_norm_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_output_1\"\n",
    "compare_tensors(hf_norm_out, ff_norm_out)\n",
    "hf_lm_head_out = f\"{hf_weight_base_path}/fwd_step_0_base_model.model.lm_head.output_0\"\n",
    "ff_lm_head_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_output_0\"\n",
    "compare_tensors(hf_lm_head_out, ff_lm_head_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "\n",
    "ff_BWD_softmax_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_100_layer-name_Softmax_shard-id_0_input_0\"\n",
    "\n",
    "hf_BWD_lm_head_out = f\"{hf_weight_base_path}/bwd_step_0_base_model.model.lm_head.go_0\"\n",
    "ff_BWD_lm_head_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_output_0\"\n",
    "compare_tensors(hf_BWD_lm_head_out, ff_BWD_lm_head_out, tolerance=1e-5)\n",
    "# compare weights\n",
    "hf_lm_head_weight = f\"{hf_weight_base_path}/base_model.model.lm_head.weight\"\n",
    "ff_lm_head_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_weight_0\"\n",
    "compare_tensors(hf_lm_head_weight, ff_lm_head_weight, tolerance=1e-5)\n",
    "hf_BWD_lm_head_in = f\"{hf_weight_base_path}/bwd_step_0_base_model.model.lm_head.gi_0\"\n",
    "ff_BWD_lm_head_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_output_shard-id_0_input_0\"\n",
    "compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in, tolerance=1e-5)\n",
    "# # Manually check the matmul\n",
    "# ff_tensor_out = np.loadtxt(ff_BWD_lm_head_out, delimiter=',')\n",
    "# ff_weight = np.loadtxt(ff_lm_head_weight, delimiter=',').reshape((4096,32000), order='F')\n",
    "# ff_tensor_out = ff_tensor_out[:32000*24].reshape((32000,24), order='F')\n",
    "# print(ff_tensor_out.shape)\n",
    "# print(ff_weight.shape)\n",
    "# print(np.matmul(ff_weight, ff_tensor_out))\n",
    "# compare_tensors(hf_BWD_lm_head_in, ff_BWD_lm_head_in)\n",
    "# ff_tensor = np.loadtxt(ff_tensor_filepath, delimiter=',')\n",
    "\n",
    "hf_BWD_norm_out = f\"{hf_weight_base_path}/bwd_step_0_norm.go_0\"\n",
    "ff_BWD_norm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_output_0\"\n",
    "compare_hf_tensors(hf_BWD_lm_head_in, hf_BWD_norm_out)\n",
    "compare_tensors(hf_BWD_norm_out, ff_BWD_norm_out)\n",
    "ff_BWD_norm_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_weight_0\"\n",
    "hf_FWD_norm_weight = f\"{hf_weight_base_path}/base_model.model.model.norm.weight\"\n",
    "compare_tensors(hf_FWD_norm_weight, ff_BWD_norm_weight, tolerance=1e-5)\n",
    "hf_BWD_norm_in = f\"{hf_weight_base_path}/bwd_step_0_norm.gi_0\"\n",
    "ff_BWD_norm_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{tot_num_layers-1}_layer-name_norm_shard-id_0_input_1\"\n",
    "compare_tensors(hf_BWD_norm_in, ff_BWD_norm_in, tolerance=1e-5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Huggingface checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "\n",
      "Huggingface-FlexFlow checks:\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_bwd-step_0_layer-num_11_layer-name_SigmoidSiluMulti_shard-id_0_output_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  1.8299303e+01\n",
      "  1.3871717e+01  1.8452764e+00]\n",
      "FF:[ 6.43506250e+03 -6.48986000e+05  1.17611156e+05 ...  1.82993031e+01\n",
      "  1.38717194e+01  1.84527588e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[2394]\n",
      "Ok!\n",
      "mismatch between /usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/bwd_step_0_layers.11.mlp.down_proj.gi_0 and /usr0/home/goliaro/Desktop/FlexFlow/build/inference_tensors/model_0_bwd-step_0_layer-num_11_layer-name_layers_11_feed_forward_w2_shard-id_0_input_0\n",
      "HF: [ 6.4350547e+03 -6.4898600e+05  1.1761116e+05 ...  1.8299303e+01\n",
      "  1.3871717e+01  1.8452764e+00]\n",
      "FF:[ 6.43506250e+03 -6.48986000e+05  1.17611156e+05 ...  1.82993031e+01\n",
      "  1.38717194e+01  1.84527588e+00]\n",
      "[ True  True  True ...  True  True  True]\n",
      "[2394]\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "(64, 12, 24)\n",
      "(64, 12, 24)\n",
      "torch.Size([12, 24, 64])\n",
      "torch.Size([12, 64, 24])\n",
      "3.7760416666666665% mismatch in QK prods softmax out grad\n",
      "hf_kproj_grads_post_rotary:  (24, 64, 12)\n",
      "hf_kproj_grads_before_rotary:  (24, 64, 12)\n",
      "[[-2.1751599e-01  1.2245592e-01 -2.6237822e-01 ...  1.4371538e+00\n",
      "   5.2717543e-01  5.1425427e-01]\n",
      " [-7.6055496e+01  4.2463268e+01 -1.2235089e+02 ...  5.3328156e+02\n",
      "   2.3810944e+02  1.8990283e+02]\n",
      " [ 5.2804117e+00 -4.9826388e+00  4.6240320e+00 ... -5.4525635e+01\n",
      "  -2.1779711e+01 -3.2857445e+01]\n",
      " ...\n",
      " [ 1.0541155e+00 -3.1229946e-01  1.4272718e+00 ... -4.6509657e+00\n",
      "  -2.2930331e+00  2.1488833e-01]\n",
      " [ 1.8427576e+00 -5.0031781e-01  2.1591802e+00 ... -8.0996408e+00\n",
      "  -6.6346103e-01  1.1487092e+00]\n",
      " [-3.9699785e-02  1.7903861e-02 -5.9658013e-02 ...  2.4856456e-01\n",
      "  -5.0553136e-02 -6.9623299e-02]]\n",
      "HF Qproj:\n",
      "torch.Size([24, 768])\n",
      "\t reshaped:  (24, 64, 12)\n",
      "[[ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "   0.0000000e+00  0.0000000e+00]\n",
      " [-2.1439369e-03  3.2949594e-03 -2.9551802e-04 ...  2.4234147e-01\n",
      "   4.3675132e-02 -9.2217997e-02]\n",
      " [ 2.9682016e+00 -4.1166668e+00 -1.5612273e+00 ...  1.8131609e+01\n",
      "  -2.7311683e+00 -2.3451160e+01]\n",
      " ...\n",
      " [ 7.9408998e+00 -1.6016111e+01  7.5070286e+00 ...  6.9805992e+01\n",
      "  -8.9288340e+00 -5.6585381e+01]\n",
      " [ 5.9755993e+00 -1.2562438e+01  9.3722830e+00 ...  5.6924896e+01\n",
      "   1.6420145e+00 -2.7360382e+01]\n",
      " [ 2.9259295e+00 -8.8997393e+00  5.6537924e+00 ...  4.0085789e+01\n",
      "  -5.5427680e+00 -3.3319279e+01]]\n",
      "FF Qproj:\n",
      "(24, 64, 12)\n",
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-2.14390800e-03  3.29491800e-03 -2.95515000e-04 ...  2.42337957e-01\n",
      "   4.36745250e-02 -9.22166630e-02]\n",
      " [ 2.96819830e+00 -4.11666203e+00 -1.56122601e+00 ...  1.81315899e+01\n",
      "  -2.73117018e+00 -2.34511394e+01]\n",
      " ...\n",
      " [ 7.94090462e+00 -1.60161247e+01  7.50703382e+00 ...  6.98059998e+01\n",
      "  -8.92883396e+00 -5.65854073e+01]\n",
      " [ 5.97561932e+00 -1.25624638e+01  9.37229633e+00 ...  5.69249115e+01\n",
      "   1.64204872e+00 -2.73603287e+01]\n",
      " [ 2.92593479e+00 -8.89975548e+00  5.65379906e+00 ...  4.00858383e+01\n",
      "  -5.54277229e+00 -3.33193245e+01]]\n",
      "hf_attn_in:  torch.Size([1, 24, 768])\n",
      "hf_attn_in:  (768, 24)\n",
      "[[-7.5252225e+06 -1.2484900e+03  5.3961243e+01 ... -3.3743629e+01\n",
      "  -2.8661375e+00 -1.2124748e+00]\n",
      " [-9.5513660e+06  1.8450066e+03  3.8372406e+02 ... -1.9933952e+01\n",
      "   1.4622488e+01 -2.4410028e+00]\n",
      " [ 1.1452265e+07  2.1254619e+03 -4.8265629e+01 ...  4.8204151e+01\n",
      "  -1.4841021e+01 -1.6505869e+01]\n",
      " ...\n",
      " [ 2.1089132e+06  2.8605874e+03  1.2375667e+03 ...  2.6102766e+01\n",
      "   3.1422745e+01  6.7668297e+01]\n",
      " [ 2.1169400e+06 -4.6361523e+02 -1.6561864e+02 ... -5.3914165e+00\n",
      "  -6.0169220e-02  2.2841328e+01]\n",
      " [ 7.3915345e+06  8.9268884e+02  5.4528040e+02 ...  6.2017624e+01\n",
      "   1.3753588e+01  5.2149849e+01]]\n",
      "ff_attn_in:  (768, 24)\n",
      "[[-7.52522050e+06 -1.24848975e+03  5.39611511e+01 ... -3.37436867e+01\n",
      "  -2.86611795e+00 -1.21241117e+00]\n",
      " [-9.55136800e+06  1.84500635e+03  3.83724091e+02 ... -1.99339561e+01\n",
      "   1.46225519e+01 -2.44094014e+00]\n",
      " [ 1.14522650e+07  2.12546313e+03 -4.82656937e+01 ...  4.82041969e+01\n",
      "  -1.48411064e+01 -1.65059376e+01]\n",
      " ...\n",
      " [ 2.10891300e+06  2.86058789e+03  1.23756726e+03 ...  2.61027851e+01\n",
      "   3.14227238e+01  6.76683807e+01]\n",
      " [ 2.11693950e+06 -4.63614868e+02 -1.65618515e+02 ... -5.39132690e+00\n",
      "  -6.02092740e-02  2.28413010e+01]\n",
      " [ 7.39153300e+06  8.92689453e+02  5.45280640e+02 ...  6.20176048e+01\n",
      "   1.37535381e+01  5.21498528e+01]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=299'>300</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mff_attn_in: \u001b[39m\u001b[39m\"\u001b[39m, ff_attn_in\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=300'>301</a>\u001b[0m \u001b[39mprint\u001b[39m(ff_attn_in)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=301'>302</a>\u001b[0m \u001b[39massert\u001b[39;00m(np\u001b[39m.\u001b[39mallclose(ff_attn_in, hf_attn_in, atol\u001b[39m=\u001b[39m\u001b[39m1e-2\u001b[39m))\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=303'>304</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=305'>306</a>\u001b[0m hf_kproj_grads_in \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mhf_weight_base_path\u001b[39m}\u001b[39;00m\u001b[39m/bwd_step_0_layers.\u001b[39m\u001b[39m{\u001b[39;00mlayer_num\u001b[39m}\u001b[39;00m\u001b[39m.self_attn.k_proj.gi_0\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_num_layers = 12\n",
    "for layer_num in range(tot_num_layers-1, -1, -1):\n",
    "    # HuggingFace filepaths\n",
    "    hf_BWD_norm_in = f\"{hf_weight_base_path}/bwd_step_0_norm.gi_0\"\n",
    "    hf_BWD_loraB_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.go_0\"\n",
    "    hf_BWD_loraB_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_B.default.gi_0\"\n",
    "    hf_BWD_loraA_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.go_0\"\n",
    "    hf_BWD_loraA_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_A.default.gi_0\"\n",
    "    hf_loraA_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    hf_loraB_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    hf_BWD_lora_dropout_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_dropout.default.go_0\"\n",
    "    hf_BWD_lora_dropout_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.lora_dropout.default.gi_0\"\n",
    "    hf_BWD_w2_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.go_0\"\n",
    "    hf_BWD_w2_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.down_proj.gi_0\"\n",
    "    hf_w2_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.weight\"\n",
    "    hf_BWD_w3_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.up_proj.go_0\"\n",
    "    hf_BWD_w3_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.up_proj.gi_0\"\n",
    "    hf_BWD_w1_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.gate_proj.go_0\"\n",
    "    hf_BWD_w1_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.gate_proj.gi_0\"\n",
    "    hf_BWD_act_fn_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.act_fn.gi_0\"\n",
    "    hf_BWD_act_fn_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.mlp.act_fn.go_0\"\n",
    "    hf_BWD_ffn_norm_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.post_attention_layernorm.go_0\"\n",
    "    hf_BWD_ffn_norm_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.post_attention_layernorm.gi_0\"\n",
    "    hf_BWD_attn_out_out = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.go_0\"\n",
    "    hf_BWD_attn_q_in = f\"{hf_weight_base_path}/bwd_step_0_layers.11.self_attn.q_proj.gi_0\"\n",
    "    hf_FWD_w1_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.gate_proj.output_0\"\n",
    "    hf_FWD_w3_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.up_proj.output_0\"\n",
    "    hf_FWD_act_fn_out = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.mlp.act_fn.output_0\"\n",
    "    hf_BWD_attn_oproj_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "    hf_attn_qproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.q_proj.weight\"\n",
    "    hf_attn_kproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.k_proj.weight\"\n",
    "    hf_attn_vproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.v_proj.weight\"\n",
    "    hf_attn_oproj_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.self_attn.o_proj.weight\"\n",
    "    # hf_BWD_attn_vproj_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.gi_0\"\n",
    "    # FlexFlow filepaths\n",
    "    ff_BWD_w2_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_output_0\"\n",
    "    ff_BWD_w2_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_input_0\"\n",
    "    ff_BWD_w2_in_pre = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_pre_input_0\"\n",
    "    ff_w2_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_weight_0\"\n",
    "    ff_BWD_ssm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_output_0\"\n",
    "    ff_BWD_ssm_in1 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_input_0\"\n",
    "    ff_BWD_ssm_in2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_input_1\"\n",
    "    ff_BWD_w3_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    ff_BWD_w3_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_input_0\"\n",
    "    ff_BWD_lora_A_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_input_0\"\n",
    "    ff_BWD_lora_B_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_output_0\"\n",
    "    ff_lora_A_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    ff_lora_B_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    ff_BWD_w1_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    ff_BWD_w1_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_input_0\"\n",
    "    ff_BWD_w1_in_pre = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_pre_input_0\"\n",
    "    ff_w1_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_weight_0\"\n",
    "    ff_BWD_ffn_norm_in1 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_input_0\"\n",
    "    ff_BWD_ffn_norm_in2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_input_1\"\n",
    "    ff_BWD_ffn_norm_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_ffn_norm_shard-id_0_output_0\"\n",
    "    ff_BWD_attn_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_output_0\"\n",
    "    ff_BWD_attn_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_input_0\"\n",
    "    ff_BWD_ssm_cached_w1_input = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_cached_w1_output\"\n",
    "    ff_BWD_ssm_cached_w3_input = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_cached_w3_output\"\n",
    "    ff_FWD_w1_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_output_0\"\n",
    "    ff_FWD_w3_out = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_output_0\"\n",
    "    ff_FWD_act_fnc_out = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_SigmoidSiluMulti_shard-id_0_act_fn_output\"\n",
    "    ff_BWD_attn_o_proj_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "    # ff_BWD_attn_v_proj_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_v_proj_in_grad\"\n",
    "    ff_attn_oproj_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_11_layer-name_layers_11_attention_shard-id_0_weight_0\"\n",
    "    # ff_attn_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "\n",
    "    # xxx = torch.load(hf_BWD_attn_out_out)\n",
    "    # xxx.detach().cpu().numpy().tofile(f\"{hf_BWD_attn_out_out}.flexflow\")\n",
    "    # print(f\"{hf_BWD_attn_out_out}.flexflow\")\n",
    "    \n",
    "    # HuggingFace checks\n",
    "    print(\"\\nHuggingface checks:\")\n",
    "    if layer_num == tot_num_layers-1:\n",
    "        compare_hf_tensors(hf_BWD_norm_in, hf_BWD_loraB_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_out, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_loraB_in, hf_BWD_loraA_out)\n",
    "    # compare_hf_tensors(hf_BWD_w3_out, hf_BWD_w2_out)\n",
    "    compare_hf_tensors(hf_BWD_act_fn_in, hf_BWD_w1_out)\n",
    "    check_hf_sum_tensors(hf_BWD_ffn_norm_out, hf_BWD_w1_in, hf_BWD_w3_in)\n",
    "    check_hf_sum_tensors(hf_BWD_attn_out_out, hf_BWD_ffn_norm_in, hf_BWD_norm_in)\n",
    "\n",
    "    # FlexFlow checks\n",
    "    print(\"\\nFlexFlow checks:\")\n",
    "    compare_flexflow_tensors(ff_BWD_w2_out, ff_BWD_lora_B_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in_pre, ff_BWD_lora_A_in)\n",
    "    compare_flexflow_tensors(ff_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in2, ff_BWD_w3_out)\n",
    "    compare_flexflow_tensors(ff_BWD_ssm_in1, ff_BWD_w1_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in, ff_BWD_ffn_norm_out)\n",
    "    compare_flexflow_tensors(ff_BWD_w1_in_pre, ff_BWD_w3_in)\n",
    "    compare_flexflow_tensors(ff_BWD_ffn_norm_in1, ff_BWD_ffn_norm_in2, max_len=24*768) # should fail\n",
    "    compare_flexflow_tensors(ff_BWD_ffn_norm_in2, ff_BWD_attn_out, max_len=24*768)\n",
    "\n",
    "    # HF-FlexFlow checks\n",
    "    print(\"\\nHuggingface-FlexFlow checks:\")\n",
    "    compare_tensors(hf_BWD_w2_out, ff_BWD_w2_out, tolerance=1e-5)\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    #print(torch.load(hf_w2_weight).shape)\n",
    "    compare_tensors(hf_loraA_weight, ff_lora_A_weight, tolerance=1e-5)\n",
    "    compare_tensors(hf_loraB_weight, ff_lora_B_weight, tolerance=1e-5)\n",
    "\n",
    "    compare_tensors(hf_BWD_loraB_out, ff_BWD_lora_B_out)\n",
    "    compare_tensors(hf_BWD_loraA_in, ff_BWD_lora_A_in)\n",
    "\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_ssm_out)\n",
    "    compare_tensors(hf_BWD_w2_in, ff_BWD_w2_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    compare_tensors_difference(hf_BWD_w1_in, ff_BWD_w1_in, ff_BWD_w1_in_pre)\n",
    "\n",
    "    compare_tensors(hf_FWD_w1_out, ff_FWD_w1_out)\n",
    "    compare_tensors(hf_FWD_w3_out, ff_FWD_w3_out)\n",
    "    compare_tensors(hf_BWD_w3_out, ff_BWD_w3_out)\n",
    "    compare_tensors(hf_BWD_w3_in, ff_BWD_w3_in)\n",
    "    compare_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "    # compare_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out)\n",
    "    # compare_tensors(hf_BWD_ffn_norm_in, ff_BWD_ffn_norm_in2)\n",
    "    # compare_tensors(hf_BWD_attn_out_out, ff_BWD_ffn_norm_in2)\n",
    "    compare_tensors(hf_BWD_attn_out_out, ff_BWD_attn_out)\n",
    "\n",
    "    # compare attn weight tensors\n",
    "    hidden_size = 768\n",
    "    qProjSize = 64\n",
    "    num_heads = 12\n",
    "    num_new_tokens = num_tokens = 24\n",
    "    ff_attn_weight_tensor = np.loadtxt(ff_attn_oproj_weight, delimiter=',')\n",
    "    ff_attn_qproj_weight_tensor = ff_attn_weight_tensor[:hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_kproj_weight_tensor = ff_attn_weight_tensor[hidden_size*qProjSize*num_heads:2*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_vproj_weight_tensor = ff_attn_weight_tensor[2*hidden_size*qProjSize*num_heads:3*hidden_size*qProjSize*num_heads].reshape((hidden_size,qProjSize*num_heads), order = 'F')\n",
    "    ff_attn_oproj_weight_tensor = ff_attn_weight_tensor[3*hidden_size*qProjSize*num_heads:].reshape((qProjSize*num_heads,hidden_size), order='F')\n",
    "    \n",
    "    hf_attn_qproj_weight_tensor = torch.load(hf_attn_qproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_kproj_weight_tensor = torch.load(hf_attn_kproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_vproj_weight_tensor = torch.load(hf_attn_vproj_weight).T.detach().cpu().numpy()\n",
    "    hf_attn_oproj_weight_tensor = torch.load(hf_attn_oproj_weight).T.detach().cpu().numpy()\n",
    "    \n",
    "    assert(np.allclose(ff_attn_qproj_weight_tensor, hf_attn_qproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_kproj_weight_tensor, hf_attn_kproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_vproj_weight_tensor, hf_attn_vproj_weight_tensor, atol=1e-5))\n",
    "    assert(np.allclose(ff_attn_oproj_weight_tensor, hf_attn_oproj_weight_tensor, atol=1e-5))\n",
    "    \n",
    "    # Compare attn outproj grad in tensors\n",
    "    compare_tensors(hf_BWD_attn_oproj_in, ff_BWD_attn_o_proj_in)\n",
    "    \n",
    "    ########### Compare value projs grads ######################\n",
    "    # 1. compare qk prods softmax\n",
    "    hf_qk_prods_softmax = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.qk_prods_softmax\"\n",
    "    ff_attn_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "    \n",
    "    hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)\n",
    "    ff_qk_prods_softmax = np.loadtxt(ff_attn_qk_prods_softmax, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "\n",
    "    for head_idx in range(num_heads):\n",
    "        hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "        ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "        assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "    \n",
    "    # 2. compare attn heads grads\n",
    "    hf_attn_heads_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "    ff_attn_heads_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "\n",
    "    hf_attn_heads_grads = torch.load(hf_attn_heads_grads).T.squeeze().detach().cpu().numpy()\n",
    "    ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize*num_heads, num_new_tokens), order = 'F')\n",
    "    assert(np.allclose(ff_attn_heads_grads, hf_attn_heads_grads, atol=1e-2))\n",
    "\n",
    "    # 3. vproj grads\n",
    "    hf_vproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.go_0\"\n",
    "    ff_vproj_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_v_proj_in_grad\"\n",
    "\n",
    "    hf_vproj_grads = torch.load(hf_vproj_grads).squeeze().detach().cpu().numpy()\n",
    "    ff_vproj_grads = np.loadtxt(ff_vproj_grads, delimiter=',').reshape((num_tokens, qProjSize*num_heads), order='F')\n",
    "    assert(np.allclose(hf_vproj_grads, ff_vproj_grads, atol=1e-2))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################\n",
    "    hf_value_states = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.value_states\"\n",
    "    hf_value_states = torch.load(hf_value_states).squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "    print(hf_value_states.shape)\n",
    "    ff_value_states = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_vcache\"\n",
    "    ff_value_states = np.loadtxt(ff_value_states, delimiter=',').reshape((qProjSize, num_heads, num_tokens), order='F')\n",
    "    print(ff_value_states.shape)\n",
    "    assert(np.allclose(hf_value_states, ff_value_states, atol=1e-2))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ########## Compare key and query projs grads ##################\n",
    "    ff_devQKVPRojArray = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devQKVPRojArray\"\n",
    "    ff_devQKVPRojArray = np.loadtxt(ff_devQKVPRojArray, delimiter=',').reshape((num_tokens, qProjSize*num_heads, 3), order = 'F')\n",
    "    ff_qProjGrads = ff_devQKVPRojArray[:,:,0]\n",
    "    ff_kProjGrads = ff_devQKVPRojArray[:,:,1]\n",
    "    ff_vProjGrads = ff_devQKVPRojArray[:,:,2]\n",
    "    assert(np.allclose(ff_vProjGrads, ff_vproj_grads, atol=1e-5))\n",
    "\n",
    "    # simulate qk_prods_softmax\n",
    "    ff_attn_heads_grads = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_o_proj_in_grad\"\n",
    "    ff_attn_heads_grads = np.loadtxt(ff_attn_heads_grads, delimiter=',').reshape((qProjSize,num_heads, num_new_tokens), order = 'F')\n",
    "    ff_attn_heads_grads = torch.from_numpy(ff_attn_heads_grads)\n",
    "    ff_attn_heads_grads = ff_attn_heads_grads.permute(1,2,0)\n",
    "    ff_value_states = torch.from_numpy(ff_value_states)\n",
    "    ff_value_states = ff_value_states.permute(1,0,2)\n",
    "    print(ff_attn_heads_grads.shape)\n",
    "    print(ff_value_states.shape)\n",
    "    simulated_qk_prods_softmax_grads = torch.matmul(ff_attn_heads_grads, ff_value_states)\n",
    "    #simulated_qk_prods_softmax_grads = simulated_qk_prods_softmax_grads\n",
    "    #print(\"Simulated QK prods grads:\")\n",
    "    #print(simulated_qk_prods_softmax_grads[0,:,:])\n",
    "\n",
    "    # qk prods softmax right before softmax\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.softmax_op.go_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    # assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "    mismatches = np.where(~np.isclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2))\n",
    "    mismatches = [(mismatches[0][i],mismatches[1][i], mismatches[2][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (hf_qk_prods_softmax2.shape[0] * hf_qk_prods_softmax2.shape[1] * hf_qk_prods_softmax2.shape[2])\n",
    "    print(f\"{pct_mismatch*100}% mismatch in QK prods softmax out grad\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "\n",
    "    # qk prods softmax right after softmax\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.softmax_op.gi_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad_in\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "    \n",
    "    # qk prods softmax after mask\n",
    "    hf_qk_prods_softmax2 = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.matmul_op.go_0\"\n",
    "    hf_qk_prods_softmax2 = torch.load(hf_qk_prods_softmax2)\n",
    "    ff_qk_prods_softmax2 = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax_grad_in_masked\"\n",
    "    ff_qk_prods_softmax2 = np.loadtxt(ff_qk_prods_softmax2, delimiter=',').reshape((num_new_tokens, num_tokens, num_heads), order = 'F')\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.squeeze().permute(1,2,0)\n",
    "    hf_qk_prods_softmax2 = hf_qk_prods_softmax2.detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_qk_prods_softmax2, hf_qk_prods_softmax2, atol=1e-2))\n",
    "\n",
    "    # Compare query activation\n",
    "    hf_query_activation = f\"{hf_weight_base_path}/fwd_step_0_layers.{layer_num}.self_attn.query_activation\"\n",
    "    hf_query_activation = torch.load(hf_query_activation)\n",
    "    ff_query_activation = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_query_activation\"\n",
    "    ff_query_activation = np.loadtxt(ff_query_activation, delimiter=',').reshape((qProjSize, num_heads, num_new_tokens), order = 'F')\n",
    "    hf_query_activation = hf_query_activation.squeeze().permute(2,0,1).detach().cpu().numpy()\n",
    "    assert(np.allclose(ff_query_activation, hf_query_activation, atol=1e-2))\n",
    "    \n",
    "    # Compare FF kproj with intermediate kproj data from HF\n",
    "    hf_kproj_grads_post_rotary = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.identity_kv_post_rotary.go_0\"\n",
    "    hf_kproj_grads_post_rotary = torch.load(hf_kproj_grads_post_rotary).squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    print(\"hf_kproj_grads_post_rotary: \", hf_kproj_grads_post_rotary.shape)\n",
    "    # print(hf_kproj_grads_post_rotary[0,:,:])\n",
    "    ff_kproj = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devkproj\"\n",
    "    ff_kproj = np.loadtxt(ff_kproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads), order = 'F')\n",
    "    # print(\"ff_kproj: \", ff_kproj.shape)\n",
    "    # print(ff_kproj[:,:,0])\n",
    "    assert(np.allclose(ff_kproj, hf_kproj_grads_post_rotary, atol=1e-2))\n",
    "\n",
    "    # Compare HF before and Kproj out gradients\n",
    "    hf_kproj_grads_before_rotary = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.identity_kv_before_rotary.go_0\"\n",
    "    hf_kproj_grads_before_rotary = torch.load(hf_kproj_grads_before_rotary).squeeze().permute(1,2,0).detach().cpu().numpy()\n",
    "    print(\"hf_kproj_grads_before_rotary: \", hf_kproj_grads_before_rotary.shape)\n",
    "    print(hf_kproj_grads_before_rotary[:,:,0])\n",
    "    assert(np.allclose(hf_kproj_grads_post_rotary, hf_kproj_grads_before_rotary, atol=1e-2))\n",
    "    hf_kproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.k_proj.go_0\"\n",
    "    hf_kproj_grads = torch.load(hf_kproj_grads).squeeze()\n",
    "    #print(\"hf_kproj_grads: \", hf_kproj_grads.shape)\n",
    "    #print(hf_kproj_grads[:,:64])\n",
    "    reshaped_tensor = hf_kproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "    #print(reshaped_tensor.shape)\n",
    "    assert(np.allclose(ff_kproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "    # Compare QProj\n",
    "    hf_qproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.q_proj.go_0\"\n",
    "    hf_qproj_grads = torch.load(hf_qproj_grads).squeeze()\n",
    "    print(\"HF Qproj:\")\n",
    "    print(hf_qproj_grads.shape)\n",
    "    reshaped_tensor = hf_qproj_grads.view(24, 12, 64).transpose(1, 2).contiguous().detach().cpu().numpy()\n",
    "    print(\"\\t reshaped: \", reshaped_tensor.shape)\n",
    "    print(reshaped_tensor[:,:,0])\n",
    "    ff_qproj = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_devQKVPRojArray\"\n",
    "    ff_qproj = np.loadtxt(ff_qproj, delimiter=',').reshape((num_tokens, qProjSize, num_heads, 3), order = 'F')[:,:,:,0]\n",
    "    print(\"FF Qproj:\")\n",
    "    print(ff_qproj.shape)\n",
    "    print(ff_qproj[:,:,0])\n",
    "    assert(np.allclose(ff_qproj, reshaped_tensor, atol=1e-2))\n",
    "\n",
    "    hf_attn_in = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.input_layernorm.go_0\"\n",
    "    hf_attn_in = torch.load(hf_attn_in)\n",
    "    print(\"hf_attn_in: \", hf_attn_in.shape)\n",
    "    hf_attn_in = hf_attn_in.squeeze().T\n",
    "    hf_attn_in = hf_attn_in.detach().cpu().numpy()\n",
    "    print(\"hf_attn_in: \", hf_attn_in.shape)\n",
    "    print(hf_attn_in)\n",
    "\n",
    "    ff_attn_in = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_attn_final_grad_in\"\n",
    "    ff_attn_in = np.loadtxt(ff_attn_in, delimiter=',').reshape((768,num_tokens), order = 'F')\n",
    "    print(\"ff_attn_in: \", ff_attn_in.shape)\n",
    "    print(ff_attn_in)\n",
    "    #assert(np.allclose(ff_attn_in, hf_attn_in, atol=1e-2))\n",
    "\n",
    "    mismatches = np.where(~np.isclose(ff_attn_in, hf_attn_in))\n",
    "    mismatches = [(mismatches[0][i], mismatches[1][i]) for i in range(len(mismatches[0]))]\n",
    "    pct_mismatch = len(mismatches) / (hf_attn_in.shape[0] * hf_attn_in.shape[1])\n",
    "    print(f\"{pct_mismatch*100}% mismatch in attention input grads\")\n",
    "    assert(pct_mismatch <= 0.05)\n",
    "    \n",
    "    assert(np.allclose(hf_kproj_grads, ff_kProjGrads, atol=1e-2))\n",
    "    assert(np.allclose(hf_qproj_grads, ff_qProjGrads, atol=1e-2))\n",
    "    # print(hf_qproj_grads.shape)\n",
    "    # print(hf_kproj_grads)\n",
    "    # print()\n",
    "    # print(ff_qProjGrads)\n",
    "    # print(ff_kProjGrads.shape)\n",
    "    \n",
    "    \n",
    "\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 24, 24])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     ff_qkps \u001b[39m=\u001b[39m ff_qk_prods_softmax[:,:,head_idx]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39massert\u001b[39;00m(np\u001b[39m.\u001b[39mallclose(ff_qkps, hf_qkps, atol\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m hf_value_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(hf_value_states)\u001b[39m#.squeeze().T.detach().cpu().numpy()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgs22359.sp.cs.cmu.edu/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/alignment_tests.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(hf_value_states\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_num = 11\n",
    "hf_qk_prods_softmax = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.qk_prods_softmax\"\n",
    "ff_qk_prods_softmax = f\"{ff_weight_base_path}/model_0_bwd-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_attention_shard-id_0_qk_prods_softmax\"\n",
    "\n",
    "hf_value_states = f\"{hf_weight_base_path}/fwd_step_0_layers.11.self_attn.value_states\"\n",
    "\n",
    "hf_qk_prods_softmax = torch.load(hf_qk_prods_softmax)#.squeeze().T.detach().cpu().numpy()\n",
    "ff_qk_prods_softmax = np.loadtxt(ff_qk_prods_softmax, delimiter=',').reshape((24, 24, 12), order = 'F')\n",
    "print(hf_qk_prods_softmax.shape)\n",
    "#print(ff_qk_prods_softmax.shape)\n",
    "#print(hf_qk_prods_softmax[:,:,0])\n",
    "#print()\n",
    "#print(ff_qk_prods_softmax[:,:,0])\n",
    "\n",
    "for head_idx in range(12):\n",
    "    hf_qkps = hf_qk_prods_softmax.squeeze()[head_idx, :, :].detach().cpu().numpy()\n",
    "    ff_qkps = ff_qk_prods_softmax[:,:,head_idx]\n",
    "    assert(np.allclose(ff_qkps, hf_qkps, atol=1e-5))\n",
    "\n",
    "\n",
    "hf_value_states = torch.load(hf_value_states)#.squeeze().T.detach().cpu().numpy()\n",
    "print(hf_value_states.shape)\n",
    "attn_output = torch.matmul(hf_qk_prods_softmax, hf_value_states)\n",
    "print()\n",
    "print(attn_output.shape)\n",
    "print(attn_output.transpose(1, 2).contiguous().shape)\n",
    "print(\"Hf attn heads\")\n",
    "print(torch.load(\"/usr0/home/goliaro/Desktop/FlexFlow/tests/peft/hf_peft_tensors/fwd_step_0_layers.11.self_attn.o_proj.input_0\").shape)\n",
    "\n",
    "print(\"Attn heads grads:\")\n",
    "hf_attn_heads_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.o_proj.gi_0\"\n",
    "print(torch.load(hf_attn_heads_grads).shape)\n",
    "print(\"HF value grads:\")\n",
    "vproj_grads = f\"{hf_weight_base_path}/bwd_step_0_layers.{layer_num}.self_attn.v_proj.gi_0\"\n",
    "print(torch.load(vproj_grads).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,4)\n",
    "print(a.shape)\n",
    "print(a.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[   0.0000,    0.0000,    0.0000,  ...,    0.0000,    0.0000,\n",
      "             0.0000],\n",
      "         [  27.8890,  -21.5089,   45.8214,  ...,    5.4010,  -10.8787,\n",
      "            39.7619],\n",
      "         [  19.2197,   27.4681,  -68.7141,  ...,  102.3280,   66.7925,\n",
      "          -160.8711],\n",
      "         ...,\n",
      "         [  63.9532,   17.4273,  -29.4416,  ...,  101.6105,   67.5937,\n",
      "          -198.4432],\n",
      "         [  31.2799,   13.0724,  -44.7179,  ...,  132.4898,   42.3135,\n",
      "          -194.4037],\n",
      "         [  42.3453,  -16.2693,  -55.7386,  ...,   90.5921,   52.2032,\n",
      "          -124.1802]]], device='cuda:0')\n",
      "tensor([[[-1.1845e+06, -6.7460e+05,  7.4494e+05,  ..., -9.1441e+05,\n",
      "          -1.4912e+05,  3.5769e+06],\n",
      "         [-7.3920e+01, -7.9389e+01,  1.1027e+02,  ..., -7.3020e+01,\n",
      "          -2.3540e+01,  3.4587e+02],\n",
      "         [-5.3885e+01, -1.7373e+01, -1.9780e+01,  ...,  4.1291e+01,\n",
      "           5.5099e+01,  5.5910e+01],\n",
      "         ...,\n",
      "         [-2.1948e+01, -3.2109e+01,  2.8364e+01,  ...,  3.4321e+01,\n",
      "           5.0713e+01,  5.6592e+01],\n",
      "         [-4.4339e+01, -2.8339e+01,  1.4070e+01,  ...,  6.2797e+01,\n",
      "           3.0760e+01,  6.1743e+01],\n",
      "         [-1.6287e+01, -5.0413e+01, -1.9940e+01,  ...,  4.3766e+01,\n",
      "           4.7833e+01,  4.7295e+01]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a = \"./hf_peft_tensors/bwd_step_0_layers.11.post_attention_layernorm.gi_0\"\n",
    "b = \"./hf_peft_tensors/bwd_step_0_layers.11.self_attn.o_proj.go_0\"\n",
    "a = torch.load(a)\n",
    "b = torch.load(b)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual matmul checks\n",
    "# ff_w2_grad_out_tensor = np.loadtxt(ff_BWD_w2_out, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w2_weight_tensor = np.loadtxt(ff_w2_weight, delimiter=',').reshape((3072,768), order='F')\n",
    "# ff_w2_gradin_tensor = np.matmul(ff_w2_weight_tensor, ff_w2_grad_out_tensor).reshape((3072,128), order='F')\n",
    "\n",
    "# ff_lora_gradout_tensor = np.loadtxt(ff_BWD_lora_B_out, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_lora_A_weight_tensor = np.loadtxt(ff_lora_A_weight, delimiter=',').reshape((3072,16), order='F')\n",
    "# ff_lora_B_weight_tensor = np.loadtxt(ff_lora_B_weight, delimiter=',').reshape((16,768), order='F')\n",
    "# ff_lora_int_grad_tensor = np.matmul(ff_lora_B_weight_tensor, ff_lora_gradout_tensor)\n",
    "# ff_lora_gradint_tensor = np.matmul(ff_lora_A_weight_tensor, ff_lora_int_grad_tensor)\n",
    "\n",
    "# # ff_w2_gradin_tensor = ff_w2_gradin_tensor + ff_lora_gradint_tensor\n",
    "# #print(ff_w2_gradin_tensor[:,:24])\n",
    "# print(\"calculated LORA grad in\")\n",
    "# print(ff_lora_gradint_tensor[:,:24])\n",
    "# # ff_BWD_w2_in_pre_tensor = np.loadtxt(ff_BWD_w2_in_pre, delimiter=',').reshape((3072,128), order='F')\n",
    "# ff_BWD_lora_A_in_tensor = np.loadtxt(ff_BWD_lora_A_in, delimiter=',').reshape((3072,128), order='F')\n",
    "# print(\"FlexFlow LORA grad in\")\n",
    "# print(ff_BWD_lora_A_in_tensor[:,:24])\n",
    "# # print(ff_BWD_w2_in_pre_tensor[:,:24])\n",
    "# print(\"HF lora grad in\")\n",
    "# print(torch.load(hf_BWD_loraA_in).squeeze().T.detach().cpu().numpy())\n",
    "# compare_tensors(hf_BWD_loraA_in, ff_BWD_lora_A_in)\n",
    "\n",
    "# simulate act_fn_grad\n",
    "# ssm_out_grad_tensor = np.loadtxt(ff_BWD_ssm_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# w3_fwd_out_tensor = np.loadtxt(ff_FWD_w3_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# #print(ssm_out_grad_tensor.shape, w3_fwd_out_tensor.shape)\n",
    "# act_fn_out_check = np.multiply(ssm_out_grad_tensor, w3_fwd_out_tensor)\n",
    "# print(\"simulated act fn out - simulated\")\n",
    "# print(act_fn_out_check[:,:24])\n",
    "# print(\"simulated act fn out - HF\")\n",
    "# print(torch.load(hf_BWD_act_fn_out).detach().cpu().numpy().squeeze().T)\n",
    "\n",
    "# Simulated w3_grad\n",
    "# ssm_out_grad_tensor = np.loadtxt(ff_BWD_ssm_out, delimiter=',').reshape((3072,128), order='F')[:,:24]\n",
    "# act_fnc_out_tensor = np.loadtxt(ff_FWD_act_fnc_out, delimiter=',').reshape((3072,24), order='F')\n",
    "# w3_out_gard_check = np.multiply(ssm_out_grad_tensor, act_fnc_out_tensor)\n",
    "# print(\"simulated w3 out - FF\")\n",
    "# print(w3_out_gard_check)\n",
    "# ff_BWD_w3_out_tensor = np.loadtxt(ff_BWD_w3_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# hf_BWD_w3_out_tensor = torch.load(hf_BWD_w3_out).detach().cpu().numpy().squeeze().T\n",
    "# print(\"w3 out, FF\")\n",
    "# print(ff_BWD_w3_out_tensor[:,:24])\n",
    "# print(\"w3 out, HF\")\n",
    "# print(hf_BWD_w3_out_tensor)\n",
    "\n",
    "# print_tensors(hf_BWD_w3_out, ff_BWD_w3_out, \"w3 out\")\n",
    "# assert False\n",
    "# print()\n",
    "# print()\n",
    "# print_tensors(hf_BWD_w3_out, ff_BWD_w3_out, \"w3 out\")\n",
    "# print_tensors(hf_BWD_w3_in, ff_BWD_w3_in, \"w3 in\")\n",
    "# print_tensors(hf_BWD_w1_out, ff_BWD_w1_out, \"w1 out\")\n",
    "# print_tensors(hf_BWD_w1_in, ff_BWD_w1_in, \"w1 in\")\n",
    "# print_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out, \"ffn norm out\")\n",
    "# print_tensors(hf_BWD_ffn_norm_in, ff_BWD_ffn_norm_in2, \"ffn norm in\")\n",
    "# print()\n",
    "# ff_w1_out_tensor = np.loadtxt(ff_BWD_w1_out, delimiter=',').reshape((3072,128), order='F')\n",
    "# ff_w1_in_tensor = np.loadtxt(ff_BWD_w1_in, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w1_in_pre_tensor = np.loadtxt(ff_BWD_w1_in_pre, delimiter=',').reshape((768,128), order='F')\n",
    "# ff_w1_only_in_tensor = ff_w1_in_tensor - ff_w1_in_pre_tensor\n",
    "# ff_w1_weight_tensor = np.loadtxt(ff_w1_weight, delimiter=',').reshape((768,3072), order='F')\n",
    "# ff_w1_in_check_tensor = np.matmul(ff_w1_weight_tensor, ff_w1_out_tensor)\n",
    "# print(\"W1 in (simulated):\")\n",
    "# print(ff_w1_in_check_tensor[:,:24])\n",
    "# print(\"W1 in (FF):\")\n",
    "# print(ff_w1_only_in_tensor[:,:24])\n",
    "# print(\"W1 in (HF):\")\n",
    "# print(torch.load(hf_BWD_w1_in).squeeze().T.detach().cpu().numpy())\n",
    "\n",
    "# compare_tensors_difference(hf_BWD_w2_in, ff_BWD_w2_in, ff_BWD_lora_A_in)\n",
    "# compare_tensors(hf_BWD_w3_out, ff_BWD_w3_out)\n",
    "#compare_hf_tensors(hf_BWD_ffn_norm_in, hf_BWD_attn_out_out)\n",
    "# print(\"\\nw1 out:\")\n",
    "\n",
    "# print_tensors(hf_BWD_w1_out, ff_BWD_w1_out)\n",
    "# print(\"\\nW1 in\\n\")\n",
    "# print_tensors(hf_BWD_w1_in, ff_BWD_w1_in)\n",
    "# compare_tensors(hf_BWD_w1_in, ff_BWD_w1_in)\n",
    "# print(\"\\nffn_norm\")\n",
    "# compare_tensors(hf_BWD_ffn_norm_out, ff_BWD_ffn_norm_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n",
      "Ok!\n"
     ]
    }
   ],
   "source": [
    "for layer_num in range(12):\n",
    "    hf_lora_A_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_A.default.weight\"\n",
    "    ff_lora_A_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_A\"\n",
    "    compare_tensors(hf_lora_A_weight_fp, ff_lora_A_weight_fp, tolerance=1e-5)\n",
    "    hf_lora_B_weight_fp = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.lora_B.default.weight\"\n",
    "    ff_lora_B_weight_fp = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_lora_shard-id_0_weight_B\"\n",
    "    compare_tensors(hf_lora_B_weight_fp, ff_lora_B_weight_fp, tolerance=1e-5)\n",
    "    hf_w1_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.gate_proj.weight\"\n",
    "    ff_w1_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w1_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w1_weight, ff_w1_weight, tolerance=1e-5)\n",
    "    hf_w3_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.up_proj.weight\"\n",
    "    ff_w3_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w3_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w3_weight, ff_w3_weight, tolerance=1e-5)\n",
    "    hf_w2_weight = f\"{hf_weight_base_path}/base_model.model.model.layers.{layer_num}.mlp.down_proj.weight\"\n",
    "    ff_w2_weight = f\"{ff_weight_base_path}/model_0_decoding-step_0_layer-num_{layer_num}_layer-name_layers_{layer_num}_feed_forward_w2_shard-id_0_weight_0\"\n",
    "    compare_tensors(hf_w2_weight, ff_w2_weight, tolerance=1e-5)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
